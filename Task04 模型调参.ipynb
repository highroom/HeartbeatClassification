{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4 建模与调参\n",
    "\n",
    "此部分为零基础入门数据挖掘之心电图分类的 Task4 建模调参部分，带你来了解各种模型以及模型的评价和调参策略，欢迎大家后续多多交流。\n",
    "\n",
    "赛题：零基础入门数据挖掘 - 心电图分类预测\n",
    "\n",
    "项目地址：\n",
    "\n",
    "比赛地址：\n",
    "\n",
    "\n",
    "\n",
    "## 4.1 学习目标\n",
    "\n",
    "- 学习机器学习模型的建模过程与调参流程\n",
    "- 完成相应学习打卡任务 \n",
    "\n",
    "\n",
    "\n",
    "## 4.2 内容介绍\n",
    "\n",
    "- 逻辑回归模型：\n",
    "\n",
    "  - 理解逻辑回归模型；\n",
    "  - 逻辑回归模型的应用；\n",
    "  - 逻辑回归的优缺点；\n",
    "\n",
    "- 树模型：\n",
    "\n",
    "  - 理解树模型；\n",
    "  - 树模型的应用；\n",
    "  - 树模型的优缺点；\n",
    "\n",
    "- 集成模型\n",
    "\n",
    "  - 基于bagging思想的集成模型\n",
    "    - 随机森林模型\n",
    "  - 基于boosting思想的集成模型\n",
    "    - XGBoost模型\n",
    "    - LightGBM模型\n",
    "    - CatBoost模型\n",
    "\n",
    "- 模型对比与性能评估：\n",
    "\n",
    "  - 回归模型/树模型/集成模型；\n",
    "  - 模型评估方法；\n",
    "  - 模型评价结果；\n",
    "\n",
    "- 模型调参：\n",
    "\n",
    "  - 贪心调参方法；\n",
    "\n",
    "  - 网格调参方法；\n",
    "\n",
    "  - 贝叶斯调参方法； \n",
    "\n",
    "    \n",
    "\n",
    "## 4.3 模型相关原理介绍\n",
    "\n",
    "由于相关算法原理篇幅较长，本文推荐了一些博客与教材供初学者们进行学习。\n",
    "\n",
    "### 4.3.1 逻辑回归模型\n",
    "\n",
    "https://blog.csdn.net/han_xiaoyang/article/details/49123419\n",
    "\n",
    "### 4.3.2 决策树模型\n",
    "\n",
    "https://blog.csdn.net/c406495762/article/details/76262487\n",
    "\n",
    "### 4.3.3 GBDT模型\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/45145899\n",
    "\n",
    "### 4.3.4 XGBoost模型\n",
    "\n",
    "https://blog.csdn.net/wuzhongqiang/article/details/104854890\n",
    "\n",
    "### 4.3.5 LightGBM模型\n",
    "\n",
    "https://blog.csdn.net/wuzhongqiang/article/details/105350579\n",
    "\n",
    "### 4.3.6 Catboost模型\n",
    "\n",
    "https://mp.weixin.qq.com/s/xloTLr5NJBgBspMQtxPoFA\n",
    "\n",
    "### 4.3.7 时间序列模型(选学)\n",
    "\n",
    "RNN：https://zhuanlan.zhihu.com/p/45289691\n",
    "\n",
    "LSTM：https://zhuanlan.zhihu.com/p/83496936\n",
    "\n",
    "### 4.3.8 推荐教材：\n",
    "\n",
    "《机器学习》 https://book.douban.com/subject/26708119/\n",
    "\n",
    "《统计学习方法》 https://book.douban.com/subject/10590856/\n",
    "\n",
    "《面向机器学习的特征工程》 https://book.douban.com/subject/26826639/\n",
    "\n",
    "《信用评分模型技术与应用》https://book.douban.com/subject/1488075/\n",
    "\n",
    "《数据化风控》https://book.douban.com/subject/30282558/ \n",
    "\n",
    "\n",
    "\n",
    "## 4.4 模型对比与性能评估\n",
    "\n",
    "### 4.4.1 逻辑回归\n",
    "\n",
    "- 优点\n",
    "\n",
    "  - 训练速度较快，分类的时候，计算量仅仅只和特征的数目相关；\n",
    "  - 简单易理解，模型的可解释性非常好，从特征的权重可以看到不同的特征对最后结果的影响；\n",
    "  - 适合二分类问题，不需要缩放输入特征；\n",
    "  - 内存资源占用小，只需要存储各个维度的特征值；\n",
    "\n",
    "- 缺点\n",
    "\n",
    "  - **逻辑回归需要预先处理缺失值和异常值【可参考task3特征工程】；**\n",
    "\n",
    "  - 不能用Logistic回归去解决非线性问题，因为Logistic的决策面是线性的；\n",
    "\n",
    "  - 对多重共线性数据较为敏感，且很难处理数据不平衡的问题；\n",
    "\n",
    "  - 准确率并不是很高，因为形式非常简单，很难去拟合数据的真实分布； \n",
    "\n",
    "    \n",
    "\n",
    "### 4.4.2 决策树模型\n",
    "\n",
    "- 优点\n",
    "\n",
    "  - 简单直观，生成的决策树可以可视化展示\n",
    "  - **数据不需要预处理，不需要归一化，不需要处理缺失数据**\n",
    "  - 既可以处理离散值，也可以处理连续值\n",
    "\n",
    "- 缺点\n",
    "\n",
    "  - 决策树算法非常容易过拟合，导致泛化能力不强（可进行适当的剪枝）\n",
    "  - 采用的是贪心算法，容易得到局部最优解\n",
    "\n",
    "  \n",
    "\n",
    "### 4.4.3 集成模型集成方法（ensemble method）\n",
    "\n",
    "通过组合多个学习器来完成学习任务，通过集成方法，可以将多个弱学习器组合成一个强分类器，因此集成学习的泛化能力一般比单一分类器要好。\n",
    "\n",
    "集成方法主要包括Bagging和Boosting，Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个更加强大的分类。两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果。常见的基于Baggin思想的集成模型有：随机森林、基于Boosting思想的集成模型有：Adaboost、GBDT、XgBoost、LightGBM等。 \n",
    "\n",
    "**Baggin和Boosting的区别总结如下：**\n",
    "\n",
    "- **样本选择上：** Bagging方法的训练集是从原始集中有放回的选取，所以从原始集中选出的各轮训练集之间是独立的；而Boosting方法需要每一轮的训练集不变，只是训练集中每个样本在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整\n",
    "- **样例权重上：** Bagging方法使用均匀取样，所以每个样本的权重相等；而Boosting方法根据错误率不断调整样本的权值，错误率越大则权重越大\n",
    "- **预测函数上：** Bagging方法中所有预测函数的权重相等；而Boosting方法中每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重\n",
    "- **并行计算上：** Bagging方法中各个预测函数可以并行生成；而Boosting方法各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。 \n",
    "\n",
    "\n",
    "\n",
    "### 4.4.4 模型评估方法\n",
    "\n",
    "对于模型来说，其在训练集上面的误差我们称之为**训练误差**或者**经验误差**，而在测试集上的误差称之为**测试误差**。\n",
    "\n",
    "对于我们来说，我们更关心的是模型对于新样本的学习能力，即我们希望通过对已有样本的学习，尽可能的将所有潜在样本的普遍规律学到手，而如果模型对训练样本学的太好，则有可能把训练样本自身所具有的一些特点当做所有潜在样本的普遍特点，这时候我们就会出现**过拟合**的问题。\n",
    "\n",
    "因此我们通常将已有的数据集划分为训练集和测试集两部分，其中训练集用来训练模型，而测试集则是用来评估模型对于新样本的判别能力。\n",
    "\n",
    "**对于数据集的划分，我们通常要保证满足以下两个条件：**\n",
    "\n",
    "- 训练集和测试集的分布要与样本真实分布一致，即训练集和测试集都要保证是从样本真实分布中独立同分布采样而得；\n",
    "- 训练集和测试集要互斥 \n",
    "\n",
    "**对于数据集的划分有三种方法：留出法，交叉验证法和自助法，下面挨个介绍：**\n",
    "\n",
    "- **①留出法**\n",
    "\n",
    "  留出法是直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另一个作为测试集T。需要注意的是在划分的时候要尽可能保证数据分布的一致性，即避免因数据划分过程引入额外的偏差而对最终结果产生影响。为了保证数据分布的一致性，通常我们采用**分层采样**的方式来对数据进行采样。\n",
    "\n",
    "  **Tips：** 通常，会将数据集D中大约2/3~4/5的样本作为训练集，其余的作为测试集。 \n",
    "\n",
    "- **②交叉验证法**\n",
    "\n",
    "  **k折交叉验证**通常将数据集D分为k份，其中k-1份作为训练集，剩余的一份作为测试集，这样就可以获得k组训练/测试集，可以进行k次训练与测试，最终返回的是k个测试结果的均值。交叉验证中数据集的划分依然是依据**分层采样**的方式来进行。\n",
    "\n",
    "  对于交叉验证法，其k值的选取往往决定了评估结果的稳定性和保真性，**通常k值选取10。**\n",
    "\n",
    "  当k=1的时候，我们称之为**留一法**\n",
    "\n",
    "- **③自助法**\n",
    "\n",
    "  我们每次从数据集D中取一个样本作为训练集中的元素，然后把该样本放回，重复该行为m次，这样我们就可以得到大小为m的训练集，在这里面有的样本重复出现，有的样本则没有出现过，我们把那些没有出现过的样本作为测试集。\n",
    "\n",
    "  进行这样采样的原因是因为在D中约有36.8%的数据没有在训练集中出现过。留出法与交叉验证法都是使用**分层采样**的方式进行数据采样与划分，而自助法则是使用**有放回重复采样**的方式进行数据采样\n",
    "\n",
    "**数据集划分总结**\n",
    "\n",
    "- 对于数据量充足的时候，通常采用**留出法**或者**k折交叉验证法**来进行训练/测试集的划分；\n",
    "- 对于数据集小且难以有效划分训练/测试集时使用**自助法**；\n",
    "- 对于数据集小且可有效划分的时候最好使用**留一法**来进行划分，因为这种方法最为准确 \n",
    "\n",
    "\n",
    "\n",
    "### 4.4.5 模型评价标准\n",
    "\n",
    "对于本次比赛，我们选用自定义的abs-sum作为模型评价标准。\n",
    "\n",
    "\n",
    "\n",
    "## 4.5 代码示例\n",
    "\n",
    "### 4.5.1 导入相关关和相关设置\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "```\n",
    "\n",
    "### 4.5.2 读取数据\n",
    "\n",
    "reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间\n",
    "\n",
    "```python\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "```python\n",
    "# 读取数据\n",
    "data = pd.read_csv('data/train.csv')\n",
    "# 简单预处理\n",
    "data_list = []\n",
    "for items in data.values:\n",
    "    data_list.append([items[0]] + [float(i) for i in items[1].split(',')] + [items[2]])\n",
    "\n",
    "data = pd.DataFrame(np.array(data_list))\n",
    "data.columns = ['id'] + ['s_'+str(i) for i in range(len(data_list[0])-2)] + ['label']\n",
    "\n",
    "data = reduce_mem_usage(data)\n",
    "```\n",
    "\n",
    "```\n",
    "Memory usage of dataframe is 157.93 MB\n",
    "Memory usage after optimization is: 39.67 MB\n",
    "Decreased by 74.9%\n",
    "```\n",
    "\n",
    "### 4.5.3 简单建模\n",
    "\n",
    "> 基于树模型的算法特性，异常值、缺失值处理可以跳过，但是对于业务较为了解的同学也可以自己对缺失异常值进行处理，效果可能会更优于模型处理的结果。\n",
    ">\n",
    "> 注：以下建模的据集并未构造任何特征，直接使用原特征。本次主要任务还是模建模调参。\n",
    "\n",
    "建模之前的预操作\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "# 分离数据集，方便进行交叉验证\n",
    "X_train = data.drop(['id','label'], axis=1)\n",
    "y_train = data['label']\n",
    "\n",
    "# 5折交叉验证\n",
    "folds = 5\n",
    "seed = 2021\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "```\n",
    "\n",
    "因为树模型中没有f1-score评价指标，所以需要自定义评价指标，在模型迭代中返回验证集f1-score变化情况。\n",
    "\n",
    "```\n",
    "def f1_score_vali(preds, data_vali):\n",
    "    labels = data_vali.get_label()\n",
    "    preds = np.argmax(preds.reshape(4, -1), axis=0)\n",
    "    score_vali = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    return 'f1_score', score_vali, True\n",
    "```\n",
    "\n",
    "使用Lightgbm进行建模\n",
    "\n",
    "```python\n",
    "\"\"\"对训练集数据进行划分，分成训练集和验证集，并进行相应的操作\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "# 数据集划分\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"boosting\": 'gbdt',  \n",
    "    \"lambda_l2\": 0.1,\n",
    "    \"max_depth\": -1,\n",
    "    \"num_leaves\": 128,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"metric\": None,\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 4,\n",
    "    \"nthread\": 10,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "\"\"\"使用训练集数据进行模型训练\"\"\"\n",
    "model = lgb.train(params, \n",
    "                  train_set=train_matrix, \n",
    "                  valid_sets=valid_matrix, \n",
    "                  num_boost_round=2000, \n",
    "                  verbose_eval=50, \n",
    "                  early_stopping_rounds=200,\n",
    "                  feval=f1_score_vali)\n",
    "```\n",
    "\n",
    "```\n",
    "Training until validation scores don't improve for 200 rounds\n",
    "[50]\tvalid_0's multi_logloss: 0.0535465\tvalid_0's f1_score: 0.953675\n",
    "[100]\tvalid_0's multi_logloss: 0.0484882\tvalid_0's f1_score: 0.961373\n",
    "[150]\tvalid_0's multi_logloss: 0.0507799\tvalid_0's f1_score: 0.962653\n",
    "[200]\tvalid_0's multi_logloss: 0.0531035\tvalid_0's f1_score: 0.963224\n",
    "[250]\tvalid_0's multi_logloss: 0.0547945\tvalid_0's f1_score: 0.963721\n",
    "Early stopping, best iteration is:\n",
    "[88]\tvalid_0's multi_logloss: 0.0482441\tvalid_0's f1_score: 0.959676\n",
    "```\n",
    "\n",
    "对验证集进行预测\n",
    "\n",
    "```python\n",
    "val_pre_lgb = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "preds = np.argmax(val_pre_lgb, axis=1)\n",
    "score = f1_score(y_true=y_val, y_pred=preds, average='macro')\n",
    "print('未调参前lightgbm单模型在验证集上的f1：{}'.format(score))\n",
    "```\n",
    "\n",
    "```\n",
    "未调参前lightgbm单模型在验证集上的f1：0.9596756568138634\n",
    "```\n",
    "\n",
    "更进一步的，使用5折交叉验证进行模型性能评估\n",
    "\n",
    "```python\n",
    "\"\"\"使用lightgbm 5折交叉验证进行建模预测\"\"\"\n",
    "cv_scores = []\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print('************************************ {} ************************************'.format(str(i+1)))\n",
    "    X_train_split, y_train_split, X_val, y_val = X_train.iloc[train_index], y_train[train_index], X_train.iloc[valid_index], y_train[valid_index]\n",
    "    \n",
    "    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "    valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    params = {\n",
    "                \"learning_rate\": 0.1,\n",
    "                \"boosting\": 'gbdt',  \n",
    "                \"lambda_l2\": 0.1,\n",
    "                \"max_depth\": -1,\n",
    "                \"num_leaves\": 128,\n",
    "                \"bagging_fraction\": 0.8,\n",
    "                \"feature_fraction\": 0.8,\n",
    "                \"metric\": None,\n",
    "                \"objective\": \"multiclass\",\n",
    "                \"num_class\": 4,\n",
    "                \"nthread\": 10,\n",
    "                \"verbose\": -1,\n",
    "            }\n",
    "    \n",
    "    model = lgb.train(params, \n",
    "                      train_set=train_matrix, \n",
    "                      valid_sets=valid_matrix, \n",
    "                      num_boost_round=2000, \n",
    "                      verbose_eval=100, \n",
    "                      early_stopping_rounds=200,\n",
    "                      feval=f1_score_vali)\n",
    "    \n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    val_pred = np.argmax(val_pred, axis=1)\n",
    "    cv_scores.append(f1_score(y_true=y_val, y_pred=val_pred, average='macro'))\n",
    "    print(cv_scores)\n",
    "\n",
    "print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n",
    "print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n",
    "print(\"lgb_score_std:{}\".format(np.std(cv_scores)))\n",
    "```\n",
    "\n",
    "```\n",
    "...\n",
    "lgb_scotrainre_list:[0.9674515729721614, 0.9656700872844327, 0.9700043639844769, 0.9655663272378014, 0.9631137190307674]\n",
    "lgb_score_mean:0.9663612141019279\n",
    "lgb_score_std:0.0022854824074775683\n",
    "```\n",
    "\n",
    "### 4.5.4 模型调参\n",
    "\n",
    "- **1. 贪心调参**\n",
    "\n",
    "  先使用当前对模型影响最大的参数进行调优，达到当前参数下的模型最优化，再使用对模型影响次之的参数进行调优，如此下去，直到所有的参数调整完毕。\n",
    "\n",
    "  这个方法的缺点就是可能会调到局部最优而不是全局最优，但是只需要一步一步的进行参数最优化调试即可，容易理解。\n",
    "\n",
    "  需要注意的是在树模型中参数调整的顺序，也就是各个参数对模型的影响程度，这里列举一下日常调参过程中常用的参数和调参顺序：\n",
    "\n",
    "  - ①：max_depth、num_leaves\n",
    "  - ②：min_data_in_leaf、min_child_weight\n",
    "  - ③：bagging_fraction、 feature_fraction、bagging_freq\n",
    "  - ④：reg_lambda、reg_alpha\n",
    "  - ⑤：min_split_gain\n",
    "\n",
    "  ```python\n",
    "  from sklearn.model_selection import cross_val_score\n",
    "  # 调objective\n",
    "  best_obj = dict()\n",
    "  for obj in objective:\n",
    "      model = LGBMRegressor(objective=obj)\n",
    "      \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "      score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "      best_obj[obj] = score\n",
    "  \n",
    "  # num_leaves\n",
    "  best_leaves = dict()\n",
    "  for leaves in num_leaves:\n",
    "      model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0], num_leaves=leaves)\n",
    "      \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "      score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "      best_leaves[leaves] = score\n",
    "  \n",
    "  # max_depth\n",
    "  best_depth = dict()\n",
    "  for depth in max_depth:\n",
    "      model = LGBMRegressor(objective=min(best_obj.items(), key=lambda x:x[1])[0],\n",
    "                            num_leaves=min(best_leaves.items(), key=lambda x:x[1])[0],\n",
    "                            max_depth=depth)\n",
    "      \"\"\"预测并计算roc的相关指标\"\"\"\n",
    "      score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "      best_depth[depth] = score\n",
    "  \n",
    "  \"\"\"\n",
    "  可依次将模型的参数通过上面的方式进行调整优化，并且通过可视化观察在每一个最优参数下模型的得分情况\n",
    "  \"\"\"\n",
    "  ```\n",
    "\n",
    "  可依次将模型的参数通过上面的方式进行调整优化，并且通过可视化观察在每一个最优参数下模型的得分情况\n",
    "\n",
    "- **2. 网格搜索**\n",
    "\n",
    "  sklearn 提供GridSearchCV用于进行网格搜索，只需要把模型的参数输进去，就能给出最优化的结果和参数。相比起贪心调参，网格搜索的结果会更优，但是网格搜索只适合于小数据集，一旦数据的量级上去了，很难得出结果。\n",
    "\n",
    "  同样以Lightgbm算法为例，进行网格搜索调参：\n",
    "\n",
    "  ```python\n",
    "  \"\"\"通过网格搜索确定最优参数\"\"\"\n",
    "  from sklearn.model_selection import GridSearchCV\n",
    "  \n",
    "  def get_best_cv_params(learning_rate=0.1, n_estimators=581, num_leaves=31, max_depth=-1, bagging_fraction=1.0, \n",
    "                         feature_fraction=1.0, bagging_freq=0, min_data_in_leaf=20, min_child_weight=0.001, \n",
    "                         min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=None):\n",
    "      # 设置5折交叉验证\n",
    "      cv_fold = KFold(n_splits=5, shuffle=True, random_state=2021)\n",
    "  \n",
    "      model_lgb = lgb.LGBMClassifier(learning_rate=learning_rate,\n",
    "                                     n_estimators=n_estimators,\n",
    "                                     num_leaves=num_leaves,\n",
    "                                     max_depth=max_depth,\n",
    "                                     bagging_fraction=bagging_fraction,\n",
    "                                     feature_fraction=feature_fraction,\n",
    "                                     bagging_freq=bagging_freq,\n",
    "                                     min_data_in_leaf=min_data_in_leaf,\n",
    "                                     min_child_weight=min_child_weight,\n",
    "                                     min_split_gain=min_split_gain,\n",
    "                                     reg_lambda=reg_lambda,\n",
    "                                     reg_alpha=reg_alpha,\n",
    "                                     n_jobs= 8\n",
    "                                    )\n",
    "  \n",
    "      f1 = make_scorer(f1_score, average='micro')\n",
    "      grid_search = GridSearchCV(estimator=model_lgb, \n",
    "                                 cv=cv_fold,\n",
    "                                 param_grid=param_grid,\n",
    "                                 scoring=f1\n",
    "  \n",
    "                                )\n",
    "      grid_search.fit(X_train, y_train)\n",
    "  \n",
    "      print('模型当前最优参数为:{}'.format(grid_search.best_params_))\n",
    "      print('模型当前最优得分为:{}'.format(grid_search.best_score_))\n",
    "  ```\n",
    "\n",
    "  ```python\n",
    "  \"\"\"以下代码未运行，耗时较长，请谨慎运行，且每一步的最优参数需要在下一步进行手动更新，请注意\"\"\"\n",
    "  \n",
    "  \"\"\"\n",
    "  需要注意一下的是，除了获取上面的获取num_boost_round时候用的是原生的lightgbm（因为要用自带的cv）\n",
    "  下面配合GridSearchCV时必须使用sklearn接口的lightgbm。\n",
    "  \"\"\"\n",
    "  \"\"\"设置n_estimators 为581，调整num_leaves和max_depth，这里选择先粗调再细调\"\"\"\n",
    "  lgb_params = {'num_leaves': range(10, 80, 5), 'max_depth': range(3,10,2)}\n",
    "  get_best_cv_params(learning_rate=0.1, n_estimators=581, num_leaves=None, max_depth=None, min_data_in_leaf=20, \n",
    "                     min_child_weight=0.001,bagging_fraction=1.0, feature_fraction=1.0, bagging_freq=0, \n",
    "                     min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n",
    "  \n",
    "  \"\"\"num_leaves为30，max_depth为7，进一步细调num_leaves和max_depth\"\"\"\n",
    "  lgb_params = {'num_leaves': range(25, 35, 1), 'max_depth': range(5,9,1)}\n",
    "  get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=None, max_depth=None, min_data_in_leaf=20, \n",
    "                     min_child_weight=0.001,bagging_fraction=1.0, feature_fraction=1.0, bagging_freq=0, \n",
    "                     min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n",
    "  \n",
    "  \"\"\"\n",
    "  确定min_data_in_leaf为45，min_child_weight为0.001 ，下面进行bagging_fraction、feature_fraction和bagging_freq的调参\n",
    "  \"\"\"\n",
    "  lgb_params = {'bagging_fraction': [i/10 for i in range(5,10,1)], \n",
    "                'feature_fraction': [i/10 for i in range(5,10,1)],\n",
    "                'bagging_freq': range(0,81,10)\n",
    "               }\n",
    "  get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n",
    "                     min_child_weight=0.001,bagging_fraction=None, feature_fraction=None, bagging_freq=None, \n",
    "                     min_split_gain=0, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n",
    "  \n",
    "  \"\"\"\n",
    "  确定bagging_fraction为0.4、feature_fraction为0.6、bagging_freq为 ，下面进行reg_lambda、reg_alpha的调参\n",
    "  \"\"\"\n",
    "  lgb_params = {'reg_lambda': [0,0.001,0.01,0.03,0.08,0.3,0.5], 'reg_alpha': [0,0.001,0.01,0.03,0.08,0.3,0.5]}\n",
    "  get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n",
    "                     min_child_weight=0.001,bagging_fraction=0.9, feature_fraction=0.9, bagging_freq=40, \n",
    "                     min_split_gain=0, reg_lambda=None, reg_alpha=None, param_grid=lgb_params)\n",
    "  \n",
    "  \"\"\"\n",
    "  确定reg_lambda、reg_alpha都为0，下面进行min_split_gain的调参\n",
    "  \"\"\"\n",
    "  lgb_params = {'min_split_gain': [i/10 for i in range(0,11,1)]}\n",
    "  get_best_cv_params(learning_rate=0.1, n_estimators=85, num_leaves=29, max_depth=7, min_data_in_leaf=45, \n",
    "                     min_child_weight=0.001,bagging_fraction=0.9, feature_fraction=0.9, bagging_freq=40, \n",
    "                     min_split_gain=None, reg_lambda=0, reg_alpha=0, param_grid=lgb_params)\n",
    "  ```\n",
    "\n",
    "  ```python\n",
    "  \"\"\"\n",
    "  参数确定好了以后，我们设置一个比较小的learning_rate 0.005，来确定最终的num_boost_round\n",
    "  \"\"\"\n",
    "  # 设置5折交叉验证\n",
    "  # cv_fold = StratifiedKFold(n_splits=5, random_state=0, shuffle=True, )\n",
    "  final_params = {\n",
    "                  'boosting_type': 'gbdt',\n",
    "                  'learning_rate': 0.01,\n",
    "                  'num_leaves': 29,\n",
    "                  'max_depth': 7,\n",
    "                  'objective': 'multiclass',\n",
    "                  'num_class': 4,\n",
    "                  'min_data_in_leaf':45,\n",
    "                  'min_child_weight':0.001,\n",
    "                  'bagging_fraction': 0.9,\n",
    "                  'feature_fraction': 0.9,\n",
    "                  'bagging_freq': 40,\n",
    "                  'min_split_gain': 0,\n",
    "                  'reg_lambda':0,\n",
    "                  'reg_alpha':0,\n",
    "                  'nthread': 6\n",
    "                 }\n",
    "  \n",
    "  cv_result = lgb.cv(train_set=lgb_train,\n",
    "                     early_stopping_rounds=20,\n",
    "                     num_boost_round=5000,\n",
    "                     nfold=5,\n",
    "                     stratified=True,\n",
    "                     shuffle=True,\n",
    "                     params=final_params,\n",
    "                     feval=f1_score_vali,\n",
    "                     seed=0,\n",
    "                    )\n",
    "  ```\n",
    "\n",
    "  在实际调整过程中，可先设置一个较大的学习率（上面的例子中0.1），通过Lgb原生的cv函数进行树个数的确定，之后再通过上面的实例代码进行参数的调整优化。\n",
    "\n",
    "  最后针对最优的参数设置一个较小的学习率（例如0.05），同样通过cv函数确定树的个数，确定最终的参数。\n",
    "\n",
    "  需要注意的是，针对大数据集，上面每一层参数的调整都需要耗费较长时间，\n",
    "\n",
    "- **贝叶斯调参**\n",
    "\n",
    "  在使用之前需要先安装包bayesian-optimization，运行如下命令即可：\n",
    "\n",
    "  ```\n",
    "  pip install bayesian-optimization\n",
    "  ```\n",
    "\n",
    "  贝叶斯调参的主要思想是：给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布）。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。\n",
    "\n",
    "  贝叶斯调参的步骤如下：\n",
    "\n",
    "  - 定义优化函数(rf_cv）\n",
    "  - 建立模型\n",
    "  - 定义待优化的参数\n",
    "  - 得到优化结果，并返回要优化的分数指标\n",
    "\n",
    "  ```python\n",
    "  from sklearn.model_selection import cross_val_score\n",
    "  \n",
    "  \"\"\"定义优化函数\"\"\"\n",
    "  def rf_cv_lgb(num_leaves, max_depth, bagging_fraction, feature_fraction, bagging_freq, min_data_in_leaf, \n",
    "                min_child_weight, min_split_gain, reg_lambda, reg_alpha):\n",
    "      # 建立模型\n",
    "      model_lgb = lgb.LGBMClassifier(boosting_type='gbdt', objective='multiclass', num_class=4,\n",
    "                                     learning_rate=0.1, n_estimators=5000,\n",
    "                                     num_leaves=int(num_leaves), max_depth=int(max_depth), \n",
    "                                     bagging_fraction=round(bagging_fraction, 2), feature_fraction=round(feature_fraction, 2),\n",
    "                                     bagging_freq=int(bagging_freq), min_data_in_leaf=int(min_data_in_leaf),\n",
    "                                     min_child_weight=min_child_weight, min_split_gain=min_split_gain,\n",
    "                                     reg_lambda=reg_lambda, reg_alpha=reg_alpha,\n",
    "                                     n_jobs= 8\n",
    "                                    )\n",
    "      f1 = make_scorer(f1_score, average='micro')\n",
    "      val = cross_val_score(model_lgb, X_train_split, y_train_split, cv=5, scoring=f1).mean()\n",
    "  \n",
    "      return val\n",
    "  ```\n",
    "\n",
    "  ```python\n",
    "  from bayes_opt import BayesianOptimization\n",
    "  \"\"\"定义优化参数\"\"\"\n",
    "  bayes_lgb = BayesianOptimization(\n",
    "      rf_cv_lgb, \n",
    "      {\n",
    "          'num_leaves':(10, 200),\n",
    "          'max_depth':(3, 20),\n",
    "          'bagging_fraction':(0.5, 1.0),\n",
    "          'feature_fraction':(0.5, 1.0),\n",
    "          'bagging_freq':(0, 100),\n",
    "          'min_data_in_leaf':(10,100),\n",
    "          'min_child_weight':(0, 10),\n",
    "          'min_split_gain':(0.0, 1.0),\n",
    "          'reg_alpha':(0.0, 10),\n",
    "          'reg_lambda':(0.0, 10),\n",
    "      }\n",
    "  )\n",
    "  \n",
    "  \"\"\"开始优化\"\"\"\n",
    "  bayes_lgb.maximize(n_iter=10)\n",
    "  ```\n",
    "\n",
    "  ```\n",
    "  |   iter    |  target   | baggin... | baggin... | featur... | max_depth | min_ch... | min_da... | min_sp... | num_le... | reg_alpha | reg_la... |\n",
    "  |  1        |  0.9785   |  0.5174   |  10.78    |  0.8746   |  10.15    |  4.288    |  48.97    |  0.2337   |  42.83    |  6.551    |  9.015    |\n",
    "  |  2        |  0.9778   |  0.6777   |  41.77    |  0.5291   |  12.15    |  4.16     |  26.39    |  0.2461   |  55.78    |  6.528    |  0.6003   |\n",
    "  |  3        |  0.9745   |  0.5825   |  68.77    |  0.5932   |  8.36     |  9.296    |  77.74    |  0.7946   |  79.12    |  3.045    |  5.593    |\n",
    "  |  4        |  0.9802   |  0.9669   |  78.34    |  0.77     |  19.68    |  9.886    |  66.34    |  0.255    |  161.1    |  4.727    |  8.18     |\n",
    "  |  5        |  0.9836   |  0.9897   |  51.9     |  0.9737   |  16.82    |  2.001    |  42.1     |  0.03563  |  134.2    |  3.437    |  1.368    |\n",
    "  |  6        |  0.9749   |  0.5575   |  46.2     |  0.6518   |  15.9     |  7.817    |  34.12    |  0.341    |  153.2    |  7.144    |  7.899    |\n",
    "  |  7        |  0.9793   |  0.9644   |  55.08    |  0.9795   |  18.5     |  2.085    |  41.22    |  0.7031   |  129.9    |  3.369    |  2.717    |\n",
    "  |  8        |  0.9819   |  0.5926   |  58.23    |  0.6149   |  16.81    |  2.911    |  39.91    |  0.1699   |  137.3    |  2.685    |  2.891    |\n",
    "  |  9        |  0.983    |  0.7796   |  50.38    |  0.7261   |  17.87    |  3.499    |  37.59    |  0.1404   |  136.1    |  2.442    |  6.621    |\n",
    "  |  10       |  0.9843   |  0.638    |  49.32    |  0.9282   |  11.33    |  6.504    |  43.21    |  0.288    |  137.7    |  0.2083   |  6.966    |\n",
    "  |  11       |  0.9798   |  0.8196   |  47.05    |  0.5845   |  9.075    |  2.965    |  46.16    |  0.3984   |  131.6    |  3.634    |  2.601    |\n",
    "  |  12       |  0.9726   |  0.7688   |  37.57    |  0.9811   |  10.26    |  1.239    |  17.54    |  0.9651   |  46.5     |  8.834    |  6.276    |\n",
    "  |  13       |  0.9836   |  0.5214   |  48.3     |  0.8203   |  19.13    |  3.129    |  35.47    |  0.08455  |  138.2    |  2.345    |  9.691    |\n",
    "  |  14       |  0.9738   |  0.5617   |  45.75    |  0.8648   |  18.88    |  4.383    |  46.88    |  0.9315   |  141.8    |  4.968    |  5.563    |\n",
    "  |  15       |  0.9807   |  0.8046   |  47.05    |  0.6449   |  12.38    |  0.3744   |  41.13    |  0.6808   |  138.7    |  0.8521   |  9.461    |\n",
    "  =================================================================================================================================================\n",
    "  ```\n",
    "\n",
    "  ```python\n",
    "  \"\"\"显示优化结果\"\"\"\n",
    "  bayes_lgb.max\n",
    "  ```\n",
    "\n",
    "  ```\n",
    "  {'target': 0.9842625,\n",
    "   'params': {'bagging_fraction': 0.6379596054685973,\n",
    "    'bagging_freq': 49.319589248277715,\n",
    "    'feature_fraction': 0.9282486828608231,\n",
    "    'max_depth': 11.32826513626976,\n",
    "    'min_child_weight': 6.5044214037514845,\n",
    "    'min_data_in_leaf': 43.211716584925405,\n",
    "    'min_split_gain': 0.28802399981965143,\n",
    "    'num_leaves': 137.7332804262704,\n",
    "    'reg_alpha': 0.2082701560002398,\n",
    "    'reg_lambda': 6.966270735649479}}\n",
    "  ```\n",
    "\n",
    "  参数优化完成后，我们可以根据优化后的参数建立新的模型，降低学习率并寻找最优模型迭代次数\n",
    "\n",
    "  ```python\n",
    "  \"\"\"调整一个较小的学习率，并通过cv函数确定当前最优的迭代次数\"\"\"\n",
    "  base_params_lgb = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'objective': 'multiclass',\n",
    "                      'num_class': 4,\n",
    "                      'learning_rate': 0.01,\n",
    "                      'num_leaves': 138,\n",
    "                      'max_depth': 11,\n",
    "                      'min_data_in_leaf': 43,\n",
    "                      'min_child_weight':6.5,\n",
    "                      'bagging_fraction': 0.64,\n",
    "                      'feature_fraction': 0.93,\n",
    "                      'bagging_freq': 49,\n",
    "                      'reg_lambda': 7,\n",
    "                      'reg_alpha': 0.21,\n",
    "                      'min_split_gain': 0.288,\n",
    "                      'nthread': 10,\n",
    "                      'verbose': -1,\n",
    "  }\n",
    "  \n",
    "  cv_result_lgb = lgb.cv(\n",
    "      train_set=train_matrix,\n",
    "      early_stopping_rounds=1000, \n",
    "      num_boost_round=20000,\n",
    "      nfold=5,\n",
    "      stratified=True,\n",
    "      shuffle=True,\n",
    "      params=base_params_lgb,\n",
    "      feval=f1_score_vali,\n",
    "      seed=0\n",
    "  )\n",
    "  print('迭代次数{}'.format(len(cv_result_lgb['f1_score-mean'])))\n",
    "  print('最终模型的f1为{}'.format(max(cv_result_lgb['f1_score-mean'])))\n",
    "  ```\n",
    "\n",
    "  ```\n",
    "  迭代次数4833\n",
    "  最终模型的f1为0.961641452120875\n",
    "  ```\n",
    "\n",
    "  **模型参数已经确定，建立最终模型并对验证集进行验证**\n",
    "\n",
    "  ```python\n",
    "  import lightgbm as lgb\n",
    "  \"\"\"使用lightgbm 5折交叉验证进行建模预测\"\"\"\n",
    "  cv_scores = []\n",
    "  for i, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n",
    "      print('************************************ {} ************************************'.format(str(i+1)))\n",
    "      X_train_split, y_train_split, X_val, y_val = X_train.iloc[train_index], y_train[train_index], X_train.iloc[valid_index], y_train[valid_index]\n",
    "  \n",
    "      train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "      valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "  \n",
    "      params = {\n",
    "                  'boosting_type': 'gbdt',\n",
    "                  'objective': 'multiclass',\n",
    "                  'num_class': 4,\n",
    "                  'learning_rate': 0.01,\n",
    "                  'num_leaves': 138,\n",
    "                  'max_depth': 11,\n",
    "                  'min_data_in_leaf': 43,\n",
    "                  'min_child_weight':6.5,\n",
    "                  'bagging_fraction': 0.64,\n",
    "                  'feature_fraction': 0.93,\n",
    "                  'bagging_freq': 49,\n",
    "                  'reg_lambda': 7,\n",
    "                  'reg_alpha': 0.21,\n",
    "                  'min_split_gain': 0.288,\n",
    "                  'nthread': 10,\n",
    "                  'verbose': -1,\n",
    "      }\n",
    "  \n",
    "      model = lgb.train(params, train_set=train_matrix, num_boost_round=4833, valid_sets=valid_matrix, \n",
    "                        verbose_eval=1000, early_stopping_rounds=200, feval=f1_score_vali)\n",
    "      val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "      val_pred = np.argmax(val_pred, axis=1)\n",
    "      cv_scores.append(f1_score(y_true=y_val, y_pred=val_pred, average='macro'))\n",
    "      print(cv_scores)\n",
    "  \n",
    "  print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n",
    "  print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n",
    "  print(\"lgb_score_std:{}\".format(np.std(cv_scores)))\n",
    "  ```\n",
    "\n",
    "  ```\n",
    "  ...\n",
    "  lgb_scotrainre_list:[0.9615056903324599, 0.9597829114711733, 0.9644760387635415, 0.9622009947666585, 0.9607941521618003]\n",
    "  lgb_score_mean:0.9617519574991267\n",
    "  lgb_score_std:0.0015797109890455313\n",
    "  \n",
    "  ```\n",
    "\n",
    "- **模型调参小总结**\n",
    "\n",
    "  - 集成模型内置的cv函数可以较快的进行单一参数的调节，一般可以用来优先确定树模型的迭代次数\n",
    "\n",
    "  - 数据量较大的时候（例如本次项目的数据），网格搜索调参会特别特别慢，不建议尝试\n",
    "\n",
    "  - 集成模型中原生库和sklearn下的库部分参数不一致，需要注意，具体可以参考xgb和lgb的官方API\n",
    "\n",
    "    > [xgb原生库API](https://xgboost.readthedocs.io/en/stable/parameter.html)，[sklearn库下xgbAPI](https://xgboost.readthedocs.io/en/stable/python/python_api.html#module-xgboost.sklearn)\n",
    "    >\n",
    "    > [lgb原生库API](https://lightgbm.readthedocs.io/en/latest/Parameters.html)， [sklearn库下lgbAPI](https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html)\n",
    "\n",
    "\n",
    "\n",
    "## 4.6 经验总结\n",
    "\n",
    "在本节中，我们主要完成了建模与调参的工作，首先在建模的过程中通过划分数据集、交叉验证等方式对模型的性能进行评估验证\n",
    "\n",
    "最后我们对模型进行调参，这部分介绍了贪心调参、网格搜索调参、贝叶斯调参共三种调参手段，重点使用贝叶斯调参对本次项目进行简单优化，大家在实际操作的过程中可以参考调参思路进行优化，不必拘泥于以上教程所写的具体实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2 \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 157.93 MB\n",
      "Memory usage after optimization is: 39.67 MB\n",
      "Decreased by 74.9%\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_csv('train.csv')\n",
    "# 简单预处理\n",
    "data_list = []\n",
    "for items in data.values:\n",
    "    data_list.append([items[0]] + [float(i) for i in items[1].split(',')] + [items[2]])\n",
    "\n",
    "data = pd.DataFrame(np.array(data_list))\n",
    "data.columns = ['id'] + ['s_'+str(i) for i in range(len(data_list[0])-2)] + ['label']\n",
    "\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# 分离数据集，方便进行交叉验证\n",
    "X_train = data.drop(['id','label'], axis=1)\n",
    "y_train = data['label']\n",
    "\n",
    "# 5折交叉验证\n",
    "folds = 5\n",
    "seed = 2021\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为树模型中没有f1-score评价指标，所以需要自定义评价指标，在模型迭代中返回验证集f1-score变化情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_vali(preds, data_vali):\n",
    "    labels = data_vali.get_label()\n",
    "    preds = np.argmax(preds.reshape(4, -1), axis=0)\n",
    "    score_vali = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "    return 'f1_score', score_vali, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Lightgbm进行建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\tvalid_0's multi_logloss: 0.0452188\tvalid_0's f1_score: 0.959445\n",
      "[100]\tvalid_0's multi_logloss: 0.0395511\tvalid_0's f1_score: 0.966402\n",
      "[150]\tvalid_0's multi_logloss: 0.0401918\tvalid_0's f1_score: 0.968891\n",
      "[200]\tvalid_0's multi_logloss: 0.0413673\tvalid_0's f1_score: 0.970288\n",
      "[250]\tvalid_0's multi_logloss: 0.0425611\tvalid_0's f1_score: 0.970893\n",
      "Early stopping, best iteration is:\n",
      "[90]\tvalid_0's multi_logloss: 0.0394673\tvalid_0's f1_score: 0.965289\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "# 数据集划分\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"boosting\": 'gbdt',  \n",
    "    \"lambda_l2\": 0.1,\n",
    "    \"max_depth\": -1,\n",
    "    \"num_leaves\": 128,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"metric\": None,\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": 4,\n",
    "    \"nthread\": 10,\n",
    "    \"verbose\": -1,\n",
    "}\n",
    "\n",
    "\"\"\"使用训练集数据进行模型训练\"\"\"\n",
    "model = lgb.train(params, \n",
    "                  train_set=train_matrix, \n",
    "                  valid_sets=valid_matrix, \n",
    "                  num_boost_round=2000, \n",
    "                  verbose_eval=50, \n",
    "                  early_stopping_rounds=200,\n",
    "                  feval=f1_score_vali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对验证集进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未调参前lightgbm单模型在验证集上的f1：0.9652889517224001\n"
     ]
    }
   ],
   "source": [
    "val_pre_lgb = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "preds = np.argmax(val_pre_lgb, axis=1)\n",
    "score = f1_score(y_true=y_val, y_pred=preds, average='macro')\n",
    "print('未调参前lightgbm单模型在验证集上的f1：{}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更进一步的，使用5折交叉验证进行模型性能评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0408155\tvalid_0's f1_score: 0.966797\n",
      "[200]\tvalid_0's multi_logloss: 0.0437957\tvalid_0's f1_score: 0.971239\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's multi_logloss: 0.0406453\tvalid_0's f1_score: 0.967452\n",
      "[0.9674515729721614]\n",
      "************************************ 2 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0472933\tvalid_0's f1_score: 0.965828\n",
      "[200]\tvalid_0's multi_logloss: 0.0514952\tvalid_0's f1_score: 0.968138\n",
      "Early stopping, best iteration is:\n",
      "[87]\tvalid_0's multi_logloss: 0.0467472\tvalid_0's f1_score: 0.96567\n",
      "[0.9674515729721614, 0.9656700872844327]\n",
      "************************************ 3 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0378154\tvalid_0's f1_score: 0.971004\n",
      "[200]\tvalid_0's multi_logloss: 0.0405053\tvalid_0's f1_score: 0.973736\n",
      "Early stopping, best iteration is:\n",
      "[93]\tvalid_0's multi_logloss: 0.037734\tvalid_0's f1_score: 0.970004\n",
      "[0.9674515729721614, 0.9656700872844327, 0.9700043639844769]\n",
      "************************************ 4 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0495142\tvalid_0's f1_score: 0.967106\n",
      "[200]\tvalid_0's multi_logloss: 0.0542324\tvalid_0's f1_score: 0.969746\n",
      "Early stopping, best iteration is:\n",
      "[84]\tvalid_0's multi_logloss: 0.0490886\tvalid_0's f1_score: 0.965566\n",
      "[0.9674515729721614, 0.9656700872844327, 0.9700043639844769, 0.9655663272378014]\n",
      "************************************ 5 ************************************\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalid_0's multi_logloss: 0.0412544\tvalid_0's f1_score: 0.964054\n",
      "[200]\tvalid_0's multi_logloss: 0.0443025\tvalid_0's f1_score: 0.965507\n",
      "Early stopping, best iteration is:\n",
      "[96]\tvalid_0's multi_logloss: 0.0411855\tvalid_0's f1_score: 0.963114\n",
      "[0.9674515729721614, 0.9656700872844327, 0.9700043639844769, 0.9655663272378014, 0.9631137190307674]\n",
      "lgb_scotrainre_list:[0.9674515729721614, 0.9656700872844327, 0.9700043639844769, 0.9655663272378014, 0.9631137190307674]\n",
      "lgb_score_mean:0.9663612141019279\n",
      "lgb_score_std:0.0022854824074775683\n"
     ]
    }
   ],
   "source": [
    "\"\"\"使用lightgbm 5折交叉验证进行建模预测\"\"\"\n",
    "cv_scores = []\n",
    "for i, (train_index, valid_index) in enumerate(kf.split(X_train, y_train)):\n",
    "    print('************************************ {} ************************************'.format(str(i+1)))\n",
    "    X_train_split, y_train_split, X_val, y_val = X_train.iloc[train_index], y_train[train_index], X_train.iloc[valid_index], y_train[valid_index]\n",
    "    \n",
    "    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)\n",
    "    valid_matrix = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    params = {\n",
    "                \"learning_rate\": 0.1,\n",
    "                \"boosting\": 'gbdt',  \n",
    "                \"lambda_l2\": 0.1,\n",
    "                \"max_depth\": -1,\n",
    "                \"num_leaves\": 128,\n",
    "                \"bagging_fraction\": 0.8,\n",
    "                \"feature_fraction\": 0.8,\n",
    "                \"metric\": None,\n",
    "                \"objective\": \"multiclass\",\n",
    "                \"num_class\": 4,\n",
    "                \"nthread\": 10,\n",
    "                \"verbose\": -1,\n",
    "            }\n",
    "    \n",
    "    model = lgb.train(params, \n",
    "                      train_set=train_matrix, \n",
    "                      valid_sets=valid_matrix, \n",
    "                      num_boost_round=2000, \n",
    "                      verbose_eval=100, \n",
    "                      early_stopping_rounds=200,\n",
    "                      feval=f1_score_vali)\n",
    "    \n",
    "    val_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "    \n",
    "    val_pred = np.argmax(val_pred, axis=1)\n",
    "    cv_scores.append(f1_score(y_true=y_val, y_pred=val_pred, average='macro'))\n",
    "    print(cv_scores)\n",
    "\n",
    "print(\"lgb_scotrainre_list:{}\".format(cv_scores))\n",
    "print(\"lgb_score_mean:{}\".format(np.mean(cv_scores)))\n",
    "print(\"lgb_score_std:{}\".format(np.std(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | baggin... | featur... | max_depth | min_ch... | min_da... | min_sp... | num_le... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "[LightGBM] [Warning] feature_fraction is set=0.63, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.63\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.51, subsample=1.0 will be ignored. Current value: bagging_fraction=0.51\n",
      "[LightGBM] [Warning] bagging_freq is set=51, subsample_freq=0 will be ignored. Current value: bagging_freq=51\n",
      "[LightGBM] [Warning] feature_fraction is set=0.63, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.63\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.51, subsample=1.0 will be ignored. Current value: bagging_fraction=0.51\n",
      "[LightGBM] [Warning] bagging_freq is set=51, subsample_freq=0 will be ignored. Current value: bagging_freq=51\n",
      "[LightGBM] [Warning] feature_fraction is set=0.63, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.63\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.51, subsample=1.0 will be ignored. Current value: bagging_fraction=0.51\n",
      "[LightGBM] [Warning] bagging_freq is set=51, subsample_freq=0 will be ignored. Current value: bagging_freq=51\n",
      "[LightGBM] [Warning] feature_fraction is set=0.63, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.63\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=73, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=73\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.51, subsample=1.0 will be ignored. Current value: bagging_fraction=0.51\n",
      "[LightGBM] [Warning] bagging_freq is set=51, subsample_freq=0 will be ignored. Current value: bagging_freq=51\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9732  \u001b[0m | \u001b[0m 0.5141  \u001b[0m | \u001b[0m 51.88   \u001b[0m | \u001b[0m 0.6273  \u001b[0m | \u001b[0m 11.6    \u001b[0m | \u001b[0m 1.714   \u001b[0m | \u001b[0m 73.41   \u001b[0m | \u001b[0m 0.9414  \u001b[0m | \u001b[0m 24.77   \u001b[0m | \u001b[0m 2.661   \u001b[0m | \u001b[0m 9.746   \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.91, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.91\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.98, subsample=1.0 will be ignored. Current value: bagging_fraction=0.98\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.91, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.91\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.98, subsample=1.0 will be ignored. Current value: bagging_fraction=0.98\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.91, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.91\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.98, subsample=1.0 will be ignored. Current value: bagging_fraction=0.98\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.91, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.91\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.98, subsample=1.0 will be ignored. Current value: bagging_fraction=0.98\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[LightGBM] [Warning] feature_fraction is set=0.91, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.91\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=19, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=19\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.98, subsample=1.0 will be ignored. Current value: bagging_fraction=0.98\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.9742  \u001b[0m | \u001b[95m 0.9834  \u001b[0m | \u001b[95m 5.871   \u001b[0m | \u001b[95m 0.9078  \u001b[0m | \u001b[95m 10.25   \u001b[0m | \u001b[95m 6.261   \u001b[0m | \u001b[95m 19.28   \u001b[0m | \u001b[95m 0.9082  \u001b[0m | \u001b[95m 11.99   \u001b[0m | \u001b[95m 2.843   \u001b[0m | \u001b[95m 9.486   \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.62, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.62\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.94, subsample=1.0 will be ignored. Current value: bagging_fraction=0.94\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.62, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.62\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.94, subsample=1.0 will be ignored. Current value: bagging_fraction=0.94\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.62, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.62\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.94, subsample=1.0 will be ignored. Current value: bagging_fraction=0.94\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.62, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.62\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.94, subsample=1.0 will be ignored. Current value: bagging_fraction=0.94\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "[LightGBM] [Warning] feature_fraction is set=0.62, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.62\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.94, subsample=1.0 will be ignored. Current value: bagging_fraction=0.94\n",
      "[LightGBM] [Warning] bagging_freq is set=11, subsample_freq=0 will be ignored. Current value: bagging_freq=11\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9806  \u001b[0m | \u001b[95m 0.9382  \u001b[0m | \u001b[95m 11.85   \u001b[0m | \u001b[95m 0.6163  \u001b[0m | \u001b[95m 6.3     \u001b[0m | \u001b[95m 8.158   \u001b[0m | \u001b[95m 31.47   \u001b[0m | \u001b[95m 0.7921  \u001b[0m | \u001b[95m 101.7   \u001b[0m | \u001b[95m 0.04127 \u001b[0m | \u001b[95m 0.2531  \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=92, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=92\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.73, subsample=1.0 will be ignored. Current value: bagging_fraction=0.73\n",
      "[LightGBM] [Warning] bagging_freq is set=30, subsample_freq=0 will be ignored. Current value: bagging_freq=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=92, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=92\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.73, subsample=1.0 will be ignored. Current value: bagging_fraction=0.73\n",
      "[LightGBM] [Warning] bagging_freq is set=30, subsample_freq=0 will be ignored. Current value: bagging_freq=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=92, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=92\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.73, subsample=1.0 will be ignored. Current value: bagging_fraction=0.73\n",
      "[LightGBM] [Warning] bagging_freq is set=30, subsample_freq=0 will be ignored. Current value: bagging_freq=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=92, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=92\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.73, subsample=1.0 will be ignored. Current value: bagging_fraction=0.73\n",
      "[LightGBM] [Warning] bagging_freq is set=30, subsample_freq=0 will be ignored. Current value: bagging_freq=30\n",
      "[LightGBM] [Warning] feature_fraction is set=0.74, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.74\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=92, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=92\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.73, subsample=1.0 will be ignored. Current value: bagging_fraction=0.73\n",
      "[LightGBM] [Warning] bagging_freq is set=30, subsample_freq=0 will be ignored. Current value: bagging_freq=30\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.9778  \u001b[0m | \u001b[0m 0.7346  \u001b[0m | \u001b[0m 30.86   \u001b[0m | \u001b[0m 0.7393  \u001b[0m | \u001b[0m 18.7    \u001b[0m | \u001b[0m 6.037   \u001b[0m | \u001b[0m 92.1    \u001b[0m | \u001b[0m 0.8576  \u001b[0m | \u001b[0m 162.3   \u001b[0m | \u001b[0m 2.354   \u001b[0m | \u001b[0m 7.814   \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=69, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=69\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.73, subsample=1.0 will be ignored. Current value: bagging_fraction=0.73\n",
      "[LightGBM] [Warning] bagging_freq is set=66, subsample_freq=0 will be ignored. Current value: bagging_freq=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=69, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=69\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.73, subsample=1.0 will be ignored. Current value: bagging_fraction=0.73\n",
      "[LightGBM] [Warning] bagging_freq is set=66, subsample_freq=0 will be ignored. Current value: bagging_freq=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=69, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=69\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.73, subsample=1.0 will be ignored. Current value: bagging_fraction=0.73\n",
      "[LightGBM] [Warning] bagging_freq is set=66, subsample_freq=0 will be ignored. Current value: bagging_freq=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=69, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=69\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.73, subsample=1.0 will be ignored. Current value: bagging_fraction=0.73\n",
      "[LightGBM] [Warning] bagging_freq is set=66, subsample_freq=0 will be ignored. Current value: bagging_freq=66\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=69, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=69\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.73, subsample=1.0 will be ignored. Current value: bagging_fraction=0.73\n",
      "[LightGBM] [Warning] bagging_freq is set=66, subsample_freq=0 will be ignored. Current value: bagging_freq=66\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.9759  \u001b[0m | \u001b[0m 0.7311  \u001b[0m | \u001b[0m 66.75   \u001b[0m | \u001b[0m 0.6989  \u001b[0m | \u001b[0m 10.93   \u001b[0m | \u001b[0m 0.129   \u001b[0m | \u001b[0m 69.52   \u001b[0m | \u001b[0m 0.2913  \u001b[0m | \u001b[0m 127.8   \u001b[0m | \u001b[0m 8.633   \u001b[0m | \u001b[0m 1.43    \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.77, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.77\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=13, subsample_freq=0 will be ignored. Current value: bagging_freq=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.77, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.77\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=13, subsample_freq=0 will be ignored. Current value: bagging_freq=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.77, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.77\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=13, subsample_freq=0 will be ignored. Current value: bagging_freq=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.77, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.77\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=13, subsample_freq=0 will be ignored. Current value: bagging_freq=13\n",
      "[LightGBM] [Warning] feature_fraction is set=0.77, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.77\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=34, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=34\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=13, subsample_freq=0 will be ignored. Current value: bagging_freq=13\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.9812  \u001b[0m | \u001b[95m 0.9961  \u001b[0m | \u001b[95m 13.07   \u001b[0m | \u001b[95m 0.7706  \u001b[0m | \u001b[95m 10.04   \u001b[0m | \u001b[95m 9.039   \u001b[0m | \u001b[95m 34.33   \u001b[0m | \u001b[95m 0.3328  \u001b[0m | \u001b[95m 100.3   \u001b[0m | \u001b[95m 2.188   \u001b[0m | \u001b[95m 0.3579  \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=63, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=63\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9815  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 20.0    \u001b[0m | \u001b[95m 10.0    \u001b[0m | \u001b[95m 63.19   \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 104.1   \u001b[0m | \u001b[95m 10.0    \u001b[0m | \u001b[95m 0.0     \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.82, subsample=1.0 will be ignored. Current value: bagging_fraction=0.82\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.82, subsample=1.0 will be ignored. Current value: bagging_fraction=0.82\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.82, subsample=1.0 will be ignored. Current value: bagging_fraction=0.82\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.82, subsample=1.0 will be ignored. Current value: bagging_fraction=0.82\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=31, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=31\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.82, subsample=1.0 will be ignored. Current value: bagging_fraction=0.82\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9788  \u001b[0m | \u001b[0m 0.8164  \u001b[0m | \u001b[0m 0.9833  \u001b[0m | \u001b[0m 0.7032  \u001b[0m | \u001b[0m 19.39   \u001b[0m | \u001b[0m 8.815   \u001b[0m | \u001b[0m 31.4    \u001b[0m | \u001b[0m 0.2605  \u001b[0m | \u001b[0m 164.6   \u001b[0m | \u001b[0m 7.071   \u001b[0m | \u001b[0m 1.924   \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.56, subsample=1.0 will be ignored. Current value: bagging_fraction=0.56\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.56, subsample=1.0 will be ignored. Current value: bagging_fraction=0.56\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.56, subsample=1.0 will be ignored. Current value: bagging_fraction=0.56\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.56, subsample=1.0 will be ignored. Current value: bagging_fraction=0.56\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.56, subsample=1.0 will be ignored. Current value: bagging_fraction=0.56\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.986   \u001b[0m | \u001b[95m 0.5592  \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 3.0     \u001b[0m | \u001b[95m 10.0    \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 93.82   \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 10.0    \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.9869  \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 0.5     \u001b[0m | \u001b[95m 20.0    \u001b[0m | \u001b[95m 10.0    \u001b[0m | \u001b[95m 100.0   \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 72.83   \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 10.0    \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.9753  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 3.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 100.0   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 55.85   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 10.0    \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=95, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=95, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=95, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=95, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=95, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=95\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9854  \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 20.0    \u001b[0m | \u001b[0m 10.0    \u001b[0m | \u001b[0m 95.65   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 88.84   \u001b[0m | \u001b[0m 3.099   \u001b[0m | \u001b[0m 6.588   \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.67, subsample=1.0 will be ignored. Current value: bagging_fraction=0.67\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.67, subsample=1.0 will be ignored. Current value: bagging_fraction=0.67\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.67, subsample=1.0 will be ignored. Current value: bagging_fraction=0.67\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.67, subsample=1.0 will be ignored. Current value: bagging_fraction=0.67\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.95, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.95\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=98, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=98\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.67, subsample=1.0 will be ignored. Current value: bagging_fraction=0.67\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9835  \u001b[0m | \u001b[0m 0.6655  \u001b[0m | \u001b[0m 3.085   \u001b[0m | \u001b[0m 0.946   \u001b[0m | \u001b[0m 19.89   \u001b[0m | \u001b[0m 8.98    \u001b[0m | \u001b[0m 98.67   \u001b[0m | \u001b[0m 0.4911  \u001b[0m | \u001b[0m 71.15   \u001b[0m | \u001b[0m 0.3748  \u001b[0m | \u001b[0m 9.031   \u001b[0m |\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=100, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=100\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\"\"\"定义优化参数\"\"\"\n",
    "bayes_lgb = BayesianOptimization(\n",
    "    rf_cv_lgb, \n",
    "    {\n",
    "        'num_leaves':(10, 200),\n",
    "        'max_depth':(3, 20),\n",
    "        'bagging_fraction':(0.5, 1.0),\n",
    "        'feature_fraction':(0.5, 1.0),\n",
    "        'bagging_freq':(0, 100),\n",
    "        'min_data_in_leaf':(10,100),\n",
    "        'min_child_weight':(0, 10),\n",
    "        'min_split_gain':(0.0, 1.0),\n",
    "        'reg_alpha':(0.0, 10),\n",
    "        'reg_lambda':(0.0, 10),\n",
    "    }\n",
    ")\n",
    "\n",
    "\"\"\"开始优化\"\"\"\n",
    "bayes_lgb.maximize(n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
